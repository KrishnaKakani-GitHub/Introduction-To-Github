{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUMFU2bEx3+wyy5pKf5YyH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrishnaKakani-GitHub/Introduction-To-Github/blob/main/Project3_Amazon_Product_Recommendation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbx4cj1HchhI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/My Drive/MIT CLASS/ratings_Electronics.csv'\n",
        "df = pd.read_csv(file_path, header=None)\n",
        "# Display the first few rows of the dataframe\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install surprise"
      ],
      "metadata": {
        "id": "kcVKqXlUzJrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries and overview of the dataset\n",
        "\n",
        "import warnings # Used to ignore the warning given as ou\n",
        "tput of the code\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np # Basic libraries of python for numeric\n",
        "and dataframe computations\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt # Basic library for data visualization\n",
        "import seaborn as sns # Slightly advanced library for data vis\n",
        "ualization\n",
        "from collections import defaultdict # A dictionary output that does not rais\n",
        "e a key error\n",
        "from sklearn.metrics import mean_squared_error # A performance metrics in sklearn"
      ],
      "metadata": {
        "id": "yxkzXTrAzJvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "\n",
        "df.columns = ['user_id', 'prod_id', 'rating', 'timestamp'] # Adding column names\n",
        "df = df.drop('timestamp', axis = 1) # Dropping timestamp\n",
        "df_copy = df.copy(deep = True) # Copying the data to another DataFrame"
      ],
      "metadata": {
        "id": "4GQblU4TzJyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the column containing the users\n",
        "users = df.user_id\n",
        "# Create a dictionary from users to their number of ratings\n",
        "ratings_count = dict()\n",
        "for user in users:\n",
        "# If we already have the user, just add 1 to their rating count\n",
        "if user in ratings_count:\n",
        "ratings_count[user] += 1\n",
        "# Otherwise, set their rating count to 1\n",
        "else:\n",
        "ratings_count[user] = 1"
      ],
      "metadata": {
        "id": "jpUgpiVczJ2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RATINGS_CUTOFF = 50\n",
        "remove_users = []\n",
        "for user, num_ratings in ratings_count.items():\n",
        "if num_ratings < RATINGS_CUTOFF:\n",
        "remove_users.append(user)\n",
        "df = df.loc[ ~ df.user_id.isin(remove_users)]"
      ],
      "metadata": {
        "id": "E2xfXE51zJ5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prods = df.prod_id\n",
        "# Create a dictionary from products to their number of ratings\n",
        "ratings_count = dict()\n",
        "for prod in prods:\n",
        "# If we already have the product, just add 1 to its rating count\n",
        "if prod in ratings_count:\n",
        "ratings_count[prod] += 1\n",
        "# Otherwise, set their rating count to 1\n",
        "else:\n",
        "ratings_count[prod] = 1"
      ],
      "metadata": {
        "id": "hCksVwuWzJ7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We want our item to have at least 5 ratings to be considered\n",
        "RATINGS_CUTOFF = 5\n",
        "remove_users = []\n",
        "for user, num_ratings in ratings_count.items():\n",
        "if num_ratings < RATINGS_CUTOFF:\n",
        "remove_users.append(user)\n",
        "df_final = df.loc[~ df.prod_id.isin(remove_users)]"
      ],
      "metadata": {
        "id": "qYa97GeazJ-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a few rows of the imported dataset\n",
        "df_final"
      ],
      "metadata": {
        "id": "mQWJ6z8ZzKCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "# Check the number of rows and columns and provide observations\n",
        "rows, columns = df_final.shape\n",
        "print(\"No of rows: \", rows)\n",
        "print(\"No of columns: \", columns)"
      ],
      "metadata": {
        "id": "VhHZdIyIzKFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Data types and provide observations\n",
        "df_final.dtypes"
      ],
      "metadata": {
        "id": "9QY_YSprzKKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking For Missing Values\n",
        "\n",
        "# Check for missing values present and provide observations\n",
        "df_final.isnull().sum()"
      ],
      "metadata": {
        "id": "Thr0C5IzzKNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary Statistics\n",
        "\n",
        "# Summary statistics of 'rating' variable and provide observations\n",
        "df_final['rating'].describe()"
      ],
      "metadata": {
        "id": "xj6QuxvgzKQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the rating distribution\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Create the bar plot and provide observations\n",
        "plt.figure(figsize=(12, 6))\n",
        "df_final['rating'].value_counts().plot(kind='bar')\n",
        "plt.title('Rating Distribution')\n",
        "plt.xlabel('Ratings')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u0v8f0m4z0gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of unique users and items in the dataset\n",
        "\n",
        "# Number of total rows in the data and number of unique user id and product id in the data\n",
        "print('The number of observations in the final data = ', len(df_final))\n",
        "print('Number of unique USERS in Raw data = ', df_final['user_id'].nunique())\n",
        "print('Number of unique ITEMS in Raw data = ', df_final['prod_id'].nunique())"
      ],
      "metadata": {
        "id": "Knk-R29mz0jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Users with the most number of ratings\n",
        "\n",
        "# Top 10 users based on the number of ratings\n",
        "most_rated = df_final.groupby('user_id').size().sort_values(ascending = False)[:10]\n",
        "most_rated"
      ],
      "metadata": {
        "id": "0QAhdp7Jz0oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: Rank Based Recommendation System"
      ],
      "metadata": {
        "id": "MykA1okS0LOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.head()"
      ],
      "metadata": {
        "id": "UQm-kCvNz0tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average rating for each product\n",
        "average_rating = df_final.groupby('prod_id')['rating'].mean()\n",
        "\n",
        "# Calculate the count of ratings for each product\n",
        "count_rating = df_final.groupby('prod_id')['rating'].count()\n",
        "\n",
        "# Create a DataFrame with calculated average and count of ratings\n",
        "final_rating = pd.DataFrame({\n",
        "    'average_rating': average_rating,\n",
        "    'count_rating': count_rating\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by average rating in descending order\n",
        "final_rating = final_rating.sort_values(by='average_rating', ascending=False)\n",
        "\n",
        "# Display the first five records of the final_rating DataFrame\n",
        "final_rating.head()"
      ],
      "metadata": {
        "id": "X4dfKr_oz0x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_n_products(final_rating, n, min_interaction):\n",
        "    \"\"\"\n",
        "    Returns the top N products with the highest average rating,\n",
        "    considering only products with at least `min_interaction` interactions.\n",
        "    \"\"\"\n",
        "    # Finding products with minimum number of interactions\n",
        "    recommendations = final_rating.groupby('ProductId').agg({'UserId': 'count'}).reset_index()\n",
        "    recommendations = recommendations.rename(columns={'UserId': 'Total_Interactions'})\n",
        "\n",
        "    # Filtering products with at least `min_interaction` interactions\n",
        "    recommendations = recommendations[recommendations['Total_Interactions'] >= min_interaction]\n",
        "\n",
        "    # Merging with average ratings\n",
        "    avg_ratings = final_rating.groupby('ProductId')['Rating'].mean().reset_index()\n",
        "    avg_ratings = avg_ratings.rename(columns={'Rating': 'Average_Rating'})\n",
        "\n",
        "    # Joining interactions with average ratings\n",
        "    recommendations = recommendations.merge(avg_ratings, on='ProductId')\n",
        "\n",
        "    # Sorting values by average rating (descending)\n",
        "    recommendations = recommendations.sort_values(by='Average_Rating', ascending=False)\n",
        "\n",
        "    # Selecting top n products\n",
        "    top_n_recommendations = recommendations.head(n)\n",
        "\n",
        "    return top_n_recommendations['ProductId'].tolist()"
      ],
      "metadata": {
        "id": "VcOw9MB1z01D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_n_popular_products(df, n, min_interactions):\n",
        "    \"\"\"\n",
        "    Recommend top N products based on popularity (number of interactions),\n",
        "    filtering out products with fewer than `min_interactions`.\n",
        "    \"\"\"\n",
        "    # Grouping by ProductId to count total interactions\n",
        "    recommendations = df.groupby('ProductId').agg({'UserId': 'count'}).reset_index()\n",
        "    recommendations = recommendations.rename(columns={'UserId': 'Total_Interactions'})\n",
        "\n",
        "    # Filtering products with at least min_interactions interactions\n",
        "    recommendations = recommendations[recommendations['Total_Interactions'] >= min_interactions]\n",
        "\n",
        "    # Sorting values by total interactions in descending order (popularity)\n",
        "    recommendations = recommendations.sort_values(by='Total_Interactions', ascending=False)\n",
        "\n",
        "    # Selecting top n products\n",
        "    top_n_recommendations = recommendations.head(n)\n",
        "\n",
        "    return top_n_recommendations['ProductId'].tolist()"
      ],
      "metadata": {
        "id": "WG94zh8c0Tv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_n_popular_products(df, n, min_interactions):\n",
        "    \"\"\"\n",
        "    Recommend top N products based on popularity (number of interactions),\n",
        "    filtering out products with fewer than `min_interactions`.\n",
        "    \"\"\"\n",
        "    # Grouping by ProductId to count total interactions\n",
        "    recommendations = df.groupby('ProductId').agg({'UserId': 'count'}).reset_index()\n",
        "    recommendations = recommendations.rename(columns={'UserId': 'Total_Interactions'})\n",
        "\n",
        "    # Filtering products with at least min_interactions interactions\n",
        "    recommendations = recommendations[recommendations['Total_Interactions'] >= min_interactions]\n",
        "\n",
        "    # Sorting values by total interactions in descending order (popularity)\n",
        "    recommendations = recommendations.sort_values(by='Total_Interactions', ascending=False)\n",
        "\n",
        "    # Selecting top n products\n",
        "    top_n_recommendations = recommendations.head(n)\n",
        "\n",
        "    return top_n_recommendations['ProductId'].tolist()"
      ],
      "metadata": {
        "id": "gMF4W_rP0Ty8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: Collaborative Filtering Recommendation System"
      ],
      "metadata": {
        "id": "VjALh8hk0bHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a baseline user-user similarity based recommendation system\n",
        "\n",
        "# To compute the accuracy of models\n",
        "from surprise import accuracy\n",
        "# Class is used to parse a file containing ratings, data should be in structure - user ;\n",
        "item ; rating\n",
        "from surprise.reader import Reader\n",
        "# Class for loading datasets\n",
        "from surprise.dataset import Dataset\n",
        "# For tuning model hyperparameters\n",
        "from surprise.model_selection import GridSearchCV\n",
        "# For splitting the rating data in train and test datasets\n",
        "from surprise.model_selection import train_test_split\n",
        "# For implementing similarity-based recommendation system\n",
        "from surprise.prediction_algorithms.knns import KNNBasic\n",
        "# For implementing matrix factorization based recommendation system\n",
        "from surprise.prediction_algorithms.matrix_factorization import SVD\n",
        "# for implementing K-Fold cross-validation\n",
        "from surprise.model_selection import KFold\n",
        "# For implementing clustering-based recommendation system\n",
        "from surprise import CoClustering"
      ],
      "metadata": {
        "id": "NiNuuiC70T2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_recall_at_k(model, testset, k=10, threshold=3.5):\n",
        "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
        "\n",
        "    # First map the predictions to each user\n",
        "    user_est_true = defaultdict(list)\n",
        "\n",
        "    # Making predictions on the test data\n",
        "    predictions = model.test(testset)\n",
        "    for uid, _, true_r, est, _ in predictions:\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "\n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Number of relevant items\n",
        "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
        "\n",
        "        # Number of recommended items in top k\n",
        "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
        "\n",
        "        # Number of relevant and recommended items in top k\n",
        "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
        "                              for (est, true_r) in user_ratings[:k])\n",
        "\n",
        "        # Precision@K: Proportion of recommended items that are relevant\n",
        "        # When n_rec_k is 0, Precision is undefined. Set to 0 to avoid division by zero\n",
        "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
        "\n",
        "        # Recall@K: Proportion of relevant items that are recommended\n",
        "        # When n_rel is 0, Recall is undefined. Set to 0 to avoid division by zero\n",
        "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
        "\n",
        "    # Mean of all the predicted precisions and recalls are calculated\n",
        "    precision = round((sum(prec for prec in precisions.values()) / len(precisions)), 3)\n",
        "    recall = round((sum(rec for rec in recalls.values()) / len(recalls)), 3)\n",
        "\n",
        "    # RMSE for model's predictions\n",
        "    accuracy.rmse(predictions)\n",
        "\n",
        "    # Print the metrics\n",
        "    print('Precision: ', precision)\n",
        "    print('Recall: ', recall)\n",
        "\n",
        "    # F1 Score calculation\n",
        "    if precision + recall > 0:\n",
        "        f1_score = round((2 * precision * recall) / (precision + recall), 3)\n",
        "    else:\n",
        "        f1_score = 0.0\n",
        "    print('F_1 score: ', f1_score)"
      ],
      "metadata": {
        "id": "LFHs1gZK0T-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.head()"
      ],
      "metadata": {
        "id": "-ZU_cp4912D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the user-user Similarity-based Recommendation System\n",
        "\n",
        "# Declaring the similarity options\n",
        "sim_options = {'name': 'cosine',\n",
        "'user_based': True}\n",
        "# Initialize the KNNBasic model using sim_options provided, Verbose = False, and setting\n",
        "random_state = 1\n",
        "sim_user_user = KNNBasic(sim_options = sim_options, random_state = 1, verbose = False)\n",
        "# Fit the model on the training data\n",
        "sim_user_user.fit(trainset)\n",
        "# Let us compute precision@k, recall@k, and f_1 score using the precision_recall_at_k fun\n",
        "ction defined above\n",
        "precision_recall_at_k(sim_user_user)"
      ],
      "metadata": {
        "id": "3dJqn71R0UCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting rating for a sample user with an interacted product\n",
        "sim_user_user.predict(\"A3LDPF5FMB782Z\", \"1400501466\", r_ui = 5, verbose = True)"
      ],
      "metadata": {
        "id": "xCTkhekw0UGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final[df_final.prod_id != \"1400501466\"].user_id.unique()"
      ],
      "metadata": {
        "id": "iFDymGb82IVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting rating for a sample user with a non interacted product\n",
        "sim_user_user.predict(\"A34BZM6S9L7QI4\", \"1400501466\", verbose = True)"
      ],
      "metadata": {
        "id": "fHDvnqQ32IYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Improving Similarity-based Recommendation System by tuning its hyperparameters\n",
        "\n",
        " # Define the Reader with the rating scale\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "# Load the data from the DataFrame\n",
        "data = Dataset.load_from_df(df[['user_id', 'prod_id', 'rating']], reader)\n",
        "# Setting up parameter grid to tune the hyperparameters\n",
        "param_grid = {\n",
        "'k': [10, 20, 30],\n",
        "'min_k': [3, 6, 9],\n",
        "'sim_options': {\n",
        "'name': ['cosine', 'msd', 'pearson', 'pearson_baseline'],\n",
        "'user_based': [True]\n",
        "}\n",
        "}\n",
        "# Performing 3-fold cross-validation to tune the hyperparameters\n",
        "gs = GridSearchCV(KNNBasic, param_grid, measures=['rmse'], cv=3, n_jobs=-1)\n",
        "# Fitting the data\n",
        "gs.fit(data)\n",
        "\n",
        "# Best RMSE score\n",
        "print(gs.best_score['rmse'])\n",
        "# Combination of parameters that gave the best RMSE score\n",
        "print(gs.best_params['rmse'])"
      ],
      "metadata": {
        "id": "79_dTCes2IcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary classes\n",
        "from surprise import Dataset, Reader, KNNBasic\n",
        "from surprise.model_selection import GridSearchCV, train_test_split\n",
        "from surprise import accuracy"
      ],
      "metadata": {
        "id": "A9sxYu1a2Ifn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-surprise"
      ],
      "metadata": {
        "id": "uj5DLyvG2Ije"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing 3-fold cross-validation to tune the hyperparameters\n",
        "gs = GridSearchCV(KNNBasic, param_grid, measures=['rmse'], cv=3, n_jobs=-1)\n",
        "\n",
        "# Fitting the data\n",
        "gs.fit(data)\n",
        "\n",
        "# Best parameters and score\n",
        "print(gs.best_params['rmse'])\n",
        "print(gs.best_score['rmse'])\n",
        "\n",
        "# Using the optimal similarity measure for user-user based collaborative filtering\n",
        "sim_options = {\n",
        "    'name': gs.best_params['rmse']['sim_options']['name'],\n",
        "    'user_based': True\n",
        "}\n",
        "\n",
        "# Splitting the data into train and test sets\n",
        "trainset, testset = train_test_split(data, test_size=0.25, random_state=42)\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
        "    user_est_true = defaultdict(list)\n",
        "\n",
        "    for uid, _, true_r, est, _ in predictions:\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "\n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
        "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
        "        n_rel_and_rec_k = sum(\n",
        "            ((true_r >= threshold) and (est >= threshold))\n",
        "            for (est, true_r) in user_ratings[:k]\n",
        "        )\n",
        "\n",
        "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
        "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
        "\n",
        "    precision_at_k = sum(prec for prec in precisions.values()) / len(precisions)\n",
        "    recall_at_k = sum(rec for rec in recalls.values()) / len(recalls)\n",
        "\n",
        "    return precision_at_k, recall_at_k"
      ],
      "metadata": {
        "id": "8QQ_Rpdj2btW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit the sim_user_user_optimized model (assuming KNNBasic is available)\n",
        "from surprise import KNNBasic\n",
        "sim_user_user_optimized = KNNBasic(sim_options=sim_options)\n",
        "sim_user_user_optimized.fit(trainset)\n",
        "\n",
        "# Use sim_user_user_optimized model to recommend for userId \"A3LDPF5FMB782Z\" and productId 1400501466\n",
        "# Predict rating for userId=\"A3LDPF5FMB782Z\" and prod_id=\"1400501466\"\n",
        "user_id = \"A3LDPF5FMB782Z\"\n",
        "prod_id = \"1400501466\"\n",
        "\n",
        "# Make the prediction\n",
        "prediction_user_item = sim_user_user_optimized.predict(user_id, prod_id)\n",
        "print(f\"Predicted rating for user {user_id} on product {prod_id}: {prediction_user_item.est:.2f}\")"
      ],
      "metadata": {
        "id": "tQdDxNr42bwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use sim_user_user_optimized model to recommend for userId \"A34BZM6S9L7QI4\" and productI\n",
        "d \"1400501466\"\n",
        "# Predict rating for userId=\"A34BZM6S9L7QI4\" and prod_id=\"1400501466\"\n",
        "user_id_new = \"A34BZM6S9L7QI4\"\n",
        "# First, check if the user has interacted with the product (not in trainset)\n",
        "if prod_id not in trainset.ur[trainset.to_inner_uid(user_id_new)]:\n",
        "# Make the prediction\n",
        "prediction_new_user_item = sim_user_user_optimized.predict(user_id_new, prod_id)\n",
        "print(f\"Predicted rating for new user {user_id_new} on product {prod_id}: {prediction\n",
        "_new_user_item.est:.2f}\")\n",
        "else:\n",
        "print(f\"User {user_id_new} has already interacted with product {prod_id}.\")"
      ],
      "metadata": {
        "id": "oWpERLkY2b27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 is the inner id of the user\n",
        "sim_user_user_optimized.get_neighbors(0, 5)"
      ],
      "metadata": {
        "id": "zpwuU2Zc2b5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing  the recommendation algorithm based on optimized KNNBasic model\n",
        "\n",
        "def get_recommendations(data, user_id, top_n, algo):\n",
        "    # Creating an empty list to store the recommended product ids\n",
        "    recommendations = []\n",
        "\n",
        "    # Creating an user item interactions matrix\n",
        "    user_item_interactions_matrix = data.pivot(index='user_id', columns='prod_id', values='rating')\n",
        "\n",
        "    # Extracting those product ids which the user_id has not interacted yet\n",
        "    non_interacted_products = user_item_interactions_matrix.loc[user_id][\n",
        "        user_item_interactions_matrix.loc[user_id].isnull()\n",
        "    ].index.tolist()\n",
        "\n",
        "    # Looping through each of the product ids which user_id has not interacted yet\n",
        "    for item_id in non_interacted_products:\n",
        "        # Predicting the ratings for those non interacted product ids by this user\n",
        "        est = algo.predict(user_id, item_id).est\n",
        "        # Appending the predicted ratings\n",
        "        recommendations.append((item_id, est))\n",
        "\n",
        "    # Sorting the predicted ratings in descending order\n",
        "    recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return recommendations[:top_n]  # Returing top n highest predicted rating products for this user\n"
      ],
      "metadata": {
        "id": "wq9H1dF72b94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting top 5 products for userId = \"A3LDPF5FMB782Z\" with similarity based recommendation system\n",
        "\n",
        "# Making top 5 recommendations for user_id \"A3LDPF5FMB782Z\" with a similarity-based recommendation engine\n",
        "recommendations = get_recommendations(df_final, \"A3LDPF5FMB782Z\", 5, sim_user_user_optimized)\n",
        "\n",
        "# Building the dataframe for above recommendations with columns \"prod_id\" and \"predicted_ratings\"\n",
        "pd.DataFrame(recommendations, columns=['prod_id', 'predicted_ratings'])\n"
      ],
      "metadata": {
        "id": "WjKHXihc3mLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Item-Item Similarity-based Collaborative Filtering Recommendation System\n",
        "\n",
        "from surprise import KNNBasic, Dataset, Reader, accuracy\n",
        "from surprise.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (assuming you have already loaded and prepared your dataset)\n",
        "df = pd.read_csv('/content/drive/My Drive/MIT CLASS/ratings_Electronics.csv', header=None)\n",
        "\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(df[[0, 1, 2]], reader)\n",
        "\n",
        "# Define similarity options\n",
        "sim_options = {\n",
        "    'name': 'cosine',  # Use cosine similarity\n",
        "    'user_based': False  # Item-item similarity\n",
        "}\n",
        "\n",
        "# Split the data into training and test sets\n",
        "trainset, testset = train_test_split(data, test_size=0.25, random_state=42)\n",
        "\n",
        "# Instantiate the KNNBasic model with similarity options\n",
        "sim_item_item = KNNBasic(sim_options=sim_options, random_state=1, verbose=False)\n",
        "\n",
        "# Train the model on the training set\n",
        "sim_item_item.fit(trainset)\n",
        "\n",
        "# Predict ratings for the test set\n",
        "predictions = sim_item_item.test(testset)\n",
        "\n",
        "# Compute precision@k, recall@k, and f1 score with k=10\n",
        "precision_at_k, recall_at_k = accuracy.precision_recall_at_k(predictions, k=10, threshold=4)\n",
        "f1_score = 2 * (precision_at_k * recall_at_k) / (precision_at_k + recall_at_k)\n",
        "\n",
        "print(f'Precision@10: {precision_at_k}')\n",
        "print(f'Recall@10: {recall_at_k}')\n",
        "print(f'F1 Score@10: {f1_score}')"
      ],
      "metadata": {
        "id": "aMkP28fH3mO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming sim_item_item is already trained and ready to use\n",
        "\n",
        "# Specify the user and product IDs for which you want to predict the rating\n",
        "user_id = 'A3LDPF5FMB782Z'\n",
        "prod_id = '1400501466'\n",
        "\n",
        "# Convert user_id and prod_id to inner IDs used by Surprise\n",
        "user_inner_id = sim_item_item.trainset.to_inner_uid(user_id)\n",
        "prod_inner_id = sim_item_item.trainset.to_inner_iid(prod_id)\n",
        "\n",
        "# Predict the rating\n",
        "rating_prediction = sim_item_item.predict(user_inner_id, prod_inner_id)\n",
        "\n",
        "# Print the predicted rating\n",
        "print(f\"Predicted rating for user {user_id} on product {prod_id}: {rating_prediction.est:.2f}\")\n"
      ],
      "metadata": {
        "id": "bPanl1wN3mcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting rating for a sample user with a non interacted product\n",
        "sim_item_item.predict(\"A34BZM6S9L7QI4\", \"1400501466\", verbose = True)"
      ],
      "metadata": {
        "id": "I8TgksA13mee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning the item-item similarity-based model"
      ],
      "metadata": {
        "id": "0lkOUUAq3mje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up parameter grid to tune the hyperparameters\n",
        "param_grid = {\n",
        "    'k': [10, 20, 30],\n",
        "    'min_k': [3]\n",
        "}\n",
        "\n",
        "# Performing 3-fold cross validation to tune the hyperparameters\n",
        "gs = GridSearchCV(KNNBasic, param_grid, measures=['rmse'], cv=3, n_jobs=-1)\n",
        "\n",
        "# Fitting the data\n",
        "gs.fit(data)\n",
        "\n",
        "# Find the best RMSE score\n",
        "print(gs.best_score['rmse'])\n",
        "\n",
        "# Find the combination of parameters that gave the best RMSE score\n",
        "print(gs.best_params['rmse'])"
      ],
      "metadata": {
        "id": "mesaiRYF3mmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import KNNBasic, Dataset, Reader, accuracy\n",
        "from surprise.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset (assuming you have already loaded and prepared your dataset)\n",
        "df = pd.read_csv('/content/drive/My Drive/MIT CLASS/ratings_Electronics.csv', header=None)\n",
        "\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(df[[0, 1, 2]], reader)\n",
        "\n",
        "# Optimal similarity options for item-item collaborative filtering\n",
        "sim_options = {\n",
        "    'name': 'msd',  # Use MSD similarity measure\n",
        "    'user_based': False  # Item-item similarity\n",
        "}\n",
        "\n",
        "# Split the data into training and test sets\n",
        "trainset, testset = train_test_split(data, test_size=0.25, random_state=42)\n",
        "\n",
        "# Creating an instance of KNNBasic with optimal hyperparameter values\n",
        "sim_item_item_optimized = KNNBasic(sim_options=sim_options, random_state=1, verbose=False)\n",
        "\n",
        "# Training the algorithm on the train set\n",
        "sim_item_item_optimized.fit(trainset)\n",
        "\n",
        "# Predictions on the test set\n",
        "predictions = sim_item_item_optimized.test(testset)\n",
        "\n",
        "# Compute evaluation metrics: precision@k, recall@k, f1_score, and RMSE\n",
        "precision_at_k, recall_at_k = accuracy.precision_recall_at_k(predictions, k=10, threshold=4)\n",
        "f1_score = 2 * (precision_at_k * recall_at_k) / (precision_at_k + recall_at_k)\n",
        "rmse = accuracy.rmse(predictions)\n",
        "\n",
        "print(f'Precision@10: {precision_at_k}')\n",
        "print(f'Recall@10: {recall_at_k}')\n",
        "print(f'F1 Score: {f1_score}')\n",
        "print(f'RMSE: {rmse}')"
      ],
      "metadata": {
        "id": "e9zAVkXc3mpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use sim_item_item_optimized model to recommend for userId \"A3LDPF5FMB782Z\" and productId \"1400501466\"\n",
        "# Assuming sim_item_item_optimized is already trained and ready to use\n",
        "\n",
        "# Specify the user and product IDs for which you want to predict the rating\n",
        "user_id = 'A3LDPF5FMB782Z'\n",
        "prod_id = '1400501466'\n",
        "\n",
        "# Convert user_id and prod_id to inner IDs used by Surprise\n",
        "user_inner_id = sim_item_item_optimized.trainset.to_inner_uid(user_id)\n",
        "prod_inner_id = sim_item_item_optimized.trainset.to_inner_iid(prod_id)\n",
        "\n",
        "# Predict the rating\n",
        "rating_prediction = sim_item_item_optimized.predict(user_inner_id, prod_inner_id)\n",
        "\n",
        "# Print the predicted rating\n",
        "print(f\"Predicted rating for user {user_id} on product {prod_id}: {rating_prediction.est:.2f}\")"
      ],
      "metadata": {
        "id": "_UdT1W8u3mxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use sim_item_item_optimized model to recommend for userId \"A34BZM6S9L7QI4\" and productId \"1400501466\"\n",
        "# Predict rating for userId=\"A34BZM6S9L7QI4\" and prod_id=\"1400501466\"\n",
        "\n",
        "user_id_new = \"A34BZM6S9L7QI4\"\n",
        "prod_id = \"1400501466\"\n",
        "\n",
        "# Convert user_id_new and prod_id to inner IDs used by Surprise\n",
        "user_inner_id_new = sim_item_item_optimized.trainset.to_inner_uid(user_id_new)\n",
        "prod_inner_id = sim_item_item_optimized.trainset.to_inner_iid(prod_id)\n",
        "\n",
        "# First, check if the user has interacted with the product (not in trainset)\n",
        "if prod_inner_id not in sim_item_item_optimized.trainset.ur[user_inner_id_new]:\n",
        "    # Make the prediction\n",
        "    prediction_new_user_item = sim_item_item_optimized.predict(user_inner_id_new, prod_inner_id)\n",
        "    print(f\"Predicted rating for new user {user_id_new} on product {prod_id}: {prediction_new_user_item.est:.2f}\")\n",
        "else:\n",
        "    print(f\"User {user_id_new} has already interacted with product {prod_id}.\")\n"
      ],
      "metadata": {
        "id": "KWVbIbAY2cAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying similar items to a given item (nearest neighbors)\n",
        "sim_item_item_optimized.get_neighbors(0, 5)"
      ],
      "metadata": {
        "id": "3o4ZKdoY2cEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making top 5 recommendations for user_id A1A5KUIIIHFF4U with similarity-based recommendation engine.\n",
        "recommendations = get_recommendations(df_final, \"A1A5KUIIIHFF4U\", 5, sim_item_item_optimized)\n"
      ],
      "metadata": {
        "id": "2PFb1kde2cHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the dataframe for above recommendations with columns \"prod_id\" and \"predicted_ratings\"\n",
        "pd.DataFrame(recommendations, columns=['prod_id', 'predicted_ratings'])"
      ],
      "metadata": {
        "id": "E3V3isU52cKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: Matrix Factorization"
      ],
      "metadata": {
        "id": "T84hKvPS4bRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using SVD matrix factorization. Use random_state = 1\n",
        "svd = SVD(random_state=1)\n",
        "\n",
        "# Train the algorithm on the train set\n",
        "svd.fit(trainset)\n",
        "\n",
        "# Training the algorithm on the train set\n",
        "svd.fit(trainset)\n",
        "\n",
        "# Use the function precision_recall_at_k to compute precision@k, recall@k, F1-Score, and RMSE\n",
        "precision_at_k, recall_at_k = accuracy.precision_recall_at_k(predictions, k=10, threshold=4)"
      ],
      "metadata": {
        "id": "-n6Zd_ds4aNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making prediction\n",
        "svd.predict(\"A3LDPF5FMB782Z\", \"1400501466\", r_ui = 5, verbose = True)"
      ],
      "metadata": {
        "id": "z0CWC71-4aP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making prediction\n",
        "svd.predict(\"A34BZM6S9L7QI4\", \"1400501466\", verbose = True)"
      ],
      "metadata": {
        "id": "uJDsM64V4aSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the parameter space to tune\n",
        "param_grid = {'n_epochs': [10, 20, 30], 'lr_all': [0.001, 0.005, 0.01],\n",
        "              'reg_all': [0.2, 0.4, 0.6]}\n",
        "\n",
        "# Performing 3-fold gridsearch cross-validation\n",
        "gs_ = GridSearchCV(, , measures=['rmse'], cv=3, n_jobs=-1)\n",
        "\n",
        "# Fitting data\n",
        "gs_.fit(data)\n",
        "\n",
        "# Best RMSE score\n",
        "print(gs_.best_score['rmse'])\n",
        "\n",
        "# Combination of parameters that gave the best RMSE score\n",
        "print(gs_.best_params['rmse'])"
      ],
      "metadata": {
        "id": "ZuzxyWqS4aVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the optimized SVD model using optimal hyperparameter search. Use random_state = 1\n",
        "svd_optimized = SVD(random_state=1)\n",
        "\n",
        "# Train the algorithm on the train set\n",
        "svd_optimized.fit(trainset)\n",
        "\n",
        "# Use the function precision_recall_at_k to compute precision@k, recall@k, F1-Score, and RMSE\n",
        "precision_at_k, recall_at_k = accuracy.precision_recall_at_k(predictions, k=10, threshold=4)"
      ],
      "metadata": {
        "id": "3ElleZYq4aYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use svd_algo_optimized model to recommend for userId \"A3LDPF5FMB782Z\" and productId \"1400501466\"\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have loaded and prepared your dataset\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/My Drive/MIT CLASS/ratings_Electronics.csv', header=None)\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(df[[0, 1, 2]], reader)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "trainset, testset = train_test_split(data, test_size=0.25, random_state=42)\n",
        "\n",
        "# Creating an instance of SVD with optimal hyperparameter values\n",
        "svd_algo_optimized = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=42)\n",
        "\n",
        "# Training the algorithm on the train set\n",
        "svd_algo_optimized.fit(trainset)\n",
        "\n",
        "# Predict rating for user A3LDPF5FMB782Z and product 1400501466\n",
        "user_id_1 = \"A3LDPF5FMB782Z\"\n",
        "prod_id_1 = \"1400501466\"\n",
        "\n",
        "# Convert user_id and prod_id to inner IDs used by Surprise\n",
        "user_inner_id_1 = svd_algo_optimized.trainset.to_inner_uid(user_id_1)\n",
        "prod_inner_id_1 = svd_algo_optimized.trainset.to_inner_iid(prod_id_1)\n",
        "\n",
        "# Predict the rating\n",
        "rating_prediction_1 = svd_algo_optimized.predict(user_inner_id_1, prod_inner"
      ],
      "metadata": {
        "id": "OTUmT0R-4rc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use svd_algo_optimized model to recommend for userId \"A34BZM6S9L7QI4\" and productId \"1400501466\"\n",
        "# Predict rating for userId=\"A34BZM6S9L7QI4\" and prod_id=\"1400501466\"\n",
        "user_id_2 = \"A34BZM6S9L7QI4\"\n",
        "prod_id_2 = \"1400501466\"\n",
        "\n",
        "# Convert user_id and prod_id to inner IDs used by Surprise\n",
        "user_inner_id_2 = svd_algo_optimized.trainset.to_inner_uid(user_id_2)\n",
        "prod_inner_id_2 = svd_algo_optimized.trainset.to_inner_iid(prod_id_2)\n",
        "\n",
        "# First, check if the user has interacted with the product (not in trainset)\n",
        "if prod_inner_id_2 not in svd_algo_optimized.trainset.ur[user_inner_id_2]:\n",
        "    # Make the prediction\n",
        "    prediction_new_user_item = svd_algo_optimized.predict(user_inner_id_2, prod_inner_id_2)\n",
        "    print(f\"Predicted rating for new user {user_id_2} on product {prod_id_2}: {prediction_new_user_item.est:.2f}\")\n",
        "else:\n",
        "    print(f\"User {user_id_2} has already interacted with product {prod_id_2}.\")"
      ],
      "metadata": {
        "id": "YCMFPejr4rfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Yl37-WR4rku"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}